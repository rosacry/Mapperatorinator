compile: true          # PyTorch 2.0 optimization
device: gpu             # Training device (cpu/gpu)
precision: 'bf16'         # Enable mixed precision (no/fp16/bf16/fp8)
seed: 42                # Project seed

checkpoint_path: ''     # Project checkpoint directory (to resume training)

data:                  # Data settings
  train_dataset_path: '/root/ORS16291'  # Training dataset directory
  train_dataset_start: 0  # Training dataset start index
  train_dataset_end: 16200    # Training dataset end index
  test_dataset_path: '/root/ORS16291'   # Testing/validation dataset directory
  test_dataset_start: 16200   # Testing/validation dataset start index
  test_dataset_end: 16291     # Testing/validation dataset end index
  src_seq_len: 32
  cycle_length: 32
  min_difficulty: 0     # Minimum difficulty to consider including in the dataset
  time_resolution: 0.1  # Steps per millisecond
  min_time: 0           # Minimum time to represent as tokens
  max_time: 1000        # Maximum time to represent as tokens
  sample_weights: ''    # Path to sample weights

dataloader:             # Dataloader settings
  num_workers: 8

optim:                  # Optimizer settings
  name: adamw
  base_lr: 1e-2         # Should be scaled with the number of devices present
  total_steps: 32768
  warmup_steps: 5000
  batch_size: 1024       # This is the batch size per GPU
  lr_scheduler: cosine
  weight_decay: 0.0
  grad_clip: 1.0
  grad_acc: 1
  final_cosine: 1e-5

eval:                   # Evaluation settings 
  every_steps: 1000
  steps: 500

checkpoint:             # Checkpoint settings
  every_steps: 10000

logging:                # Logging settings
  log_with: 'wandb'     # Logging service (wandb/tensorboard)
  every_steps: 100
  grad_l2: true
  weights_l2: true
  mode: 'online'

profile:                # Profiling settings
  do_profile: false
  early_stop: false
  wait: 8
  warmup: 8
  active: 8
  repeat: 1

hydra:
  job:
    chdir: True
  run:
    dir: ./logs/${now:%Y-%m-%d}/${now:%H-%M-%S}