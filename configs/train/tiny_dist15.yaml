defaults:
  - default
  - ../model@model: whisper_tiny_v2
  - _self_

compile: true          # PyTorch 2.0 optimization
precision: 'bf16'         # Enable mixed precision (no/fp16/bf16/fp8)
flash_attention: false  # Enable Flash Attention

model:
  do_difficulty_embed: false
  do_mapper_embed: false
  do_song_position_embed: false
  cond_size: 0

data:                  # Data settings
  dataset_type: "mmrs"
  train_dataset_path: "/workspace/datasets/MMRS39389"
  test_dataset_path: "/workspace/datasets/MMRS39389"
  train_dataset_start: 10187  # Start of ORS train dataset
  train_dataset_end: 35993  # End of ORS train dataset
  test_dataset_start: 35993
  test_dataset_end: 36664  # End of ORS test dataset
  num_classes: 152680
  # All Special Prefix Tokens
  add_out_context_types: false  # Add tokens indicating types of the out context
  add_gamemode_token: false
  add_style_token: false
  add_diff_token: true
  add_mapper_token: true
  add_year_token: false
  add_hitsounded_token: false  # Add token for whether the map has hitsounds
  add_song_length_token: true  # Add token for the length of the song
  add_global_sv_token: false  # Add token for the global slider velocity in std and ctb
  add_cs_token: false
  add_keycount_token: false  # Add token for the number of keys in mania
  add_hold_note_ratio_token: false  # Add token for the ratio of hold notes in mania
  add_scroll_speed_ratio_token: false  # Add token for the scroll speed ratio in mania
  add_descriptors: false  # Add beatmap descriptor tokens
  add_sv_special_token: false  # Add token for last SV value
  add_kiai_special_token: false  # Add token for last kiai state
  add_song_position_token: true  # Add token for the position of the song in the mapset
  # ---
  mapper_dropout_prob: 0.1
  timing_random_offset: 2
  src_seq_len: 2048
  tgt_seq_len: 2560
  rhythm_weight: 1.0    # Weight of rhythm tokens in the loss calculation
  label_smoothing: 0.1  # Label smoothing for the loss calculation
  lookback: 0.2             # Fraction of audio sequence to fill with tokens from previous inference window
  lookback_prob: 0.5  # Probability of using the lookback augmentation for a beatmap in the dataset
  context_types:       # List of context types to include in the dataset
    - "in": []
      "out": ['${context_type:timing}', '${context_type:map}']
  context_weights: [1]    # List of weights for each context type. Determines how often each context type is sampled
  mappers_path: "./datasets/beatmap_users.json"       # Path to file with all beatmap mappers
  add_timing: false      # Interweave timing tokens with the beatmap tokens
  add_snapping: true    # Model hit object snapping
  add_timing_points: false  # Model beatmap timing with timing points
  add_hitsounds: true   # Model beatmap hitsounds
  add_pre_tokens: false
  per_track: true
  add_distances: true   # Model hit object distances
  add_positions: true
  position_precision: 4 # Precision of hit object coordinates
  position_split_axes: true  # Split hit object X and Y coordinates into separate tokens
  dt_augment_prob: 0.3   # Probability of augmenting the dataset with DT
  dt_augment_range: [1., 1.2]  # Range of DT augmentation
  dt_augment_sqrt: true  # Sample DT augmentation from a square root distribution
  types_first: false       # Put the type token at the start of the group before the timeshift token
  add_kiai: false        # Model kiai times
  gamemodes: [0]  # List of gamemodes to include in the dataset
  mania_bpm_normalized_scroll_speed: true  # Normalize mania scroll speed by BPM
  add_sv: false  # Model slider velocity in std and ctb
  add_mania_sv: false  # Add mania scroll velocity in map context
  normalize_audio: true  # Normalize audio data
  frame_offset_augment_prob: 0.5  # Probability of augmenting the dataset with frame offset
  min_difficulty: 1     # Minimum difficulty to consider including in the dataset
  max_difficulty: 10    # Maximum difficulty to consider including in the dataset
  cycle_length: 1
  max_year: 2022  # Maximum year of the beatmap to include in the dataset

dataloader:             # Dataloader settings
  num_workers: 8
  drop_last: true

optim:                  # Optimizer settings
  name: muon
  base_lr: 0.004         # Should be scaled with the number of devices present
  base_lr_2: 0.001        # Secondary learning rate for the internal optimizer
  batch_size: 64
  grad_acc: 4
  total_steps: 35000
  warmup_steps: 0
  sustain_steps: 10000      # Steps to sustain the learning rate after warmup
  weight_decay: 0.01
  lr_scheduler: linear
  final_cosine: 0

